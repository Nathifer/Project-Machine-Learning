# -*- coding: utf-8 -*-
"""ProyectoFinalML_BankDataSet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Vo0GdjG-aCaiz7V5VHhz1qF4M8hM3FK

## Índice
1. [Importación de librerías](#1-importación-de-librerías)
2. [Carga del dataset](#2-carga-del-dataset)
3. [Dividir el conjunto de datos](#3-dividir-el-conjunto-de-datos)
4. [Codificar variables categóricas](#4-codificar-variables-categóricas)
5. [Estandarizar las características](#5-estandarizar-las-características)
6. [Reducir la dimensionalidad del conjunto de datos](#6-reducir-la-dimensionalidad-del-conjunto-de-datos)
"""

from google.colab import drive
drive.mount('/content/drive')

"""#Librerias"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.cluster import KMeans
from imblearn.over_sampling import RandomOverSampler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

import streamlit as st
import joblib

"""#Dataset"""

bank = pd.read_csv('/content/drive/MyDrive/bank_dataset.csv')

bank.shape

"""# Limpieza y Transformacion"""

# Seleccionar solo las columnas numéricas
df_numerical = bank.select_dtypes(include=['number'])

def obtener_outliers_iqr(df):
    outliers_dict = {}  # Diccionario para almacenar los outliers de cada columna

    for column in df.columns:
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Filtrar los valores que son outliers
        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]

        # Almacenar los outliers en el diccionario
        outliers_dict[column] = outliers.tolist()

    return outliers_dict

# Obtener los outliers para cada variable numérica
outliers_por_variable = obtener_outliers_iqr(df_numerical)

# Imprimir los outliers de cada variable
for column, outliers in outliers_por_variable.items():
    print(f'Outliers en {column}: {outliers}')

def remove_outliers(df, column, factor=3):  # Aumentamos el factor a 3
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - factor * IQR
    upper_bound = Q3 + factor * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Aplicamos el método con un factor de 3 en lugar de 1.5
bank = remove_outliers(bank, 'age', factor=3)
bank = remove_outliers(bank, 'balance', factor=3)
bank = remove_outliers(bank, 'duration', factor=3)
bank = remove_outliers(bank, 'pdays', factor=3)
bank = remove_outliers(bank, 'previous', factor=3)

def remove_outliers_percentiles(df, column, lower_percentile=0.05, upper_percentile=0.95):
    lower_bound = df[column].quantile(lower_percentile)
    upper_bound = df[column].quantile(upper_percentile)
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Aplicamos el método con percentiles del 1% al 99%
bank = remove_outliers_percentiles(bank, 'age')
bank = remove_outliers_percentiles(bank, 'balance')
bank = remove_outliers_percentiles(bank, 'duration')
bank = remove_outliers_percentiles(bank, 'pdays')
bank = remove_outliers_percentiles(bank, 'previous')

bank.shape

# Calcular la media de 'age' para cada combinación de 'job', 'marital', 'education', y 'balance'
age_mean = bank.groupby(['job', 'marital', 'education'])['age'].mean().reset_index(name='mean_age')

# Calcular la moda (la categoría más frecuente) de 'marital' para cada combinación
marital_mode = bank.groupby(['job', 'age', 'education'])['marital'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index(name='mode_marital')

# Calcular la moda de 'education' para cada combinación
education_mode = bank.groupby(['job', 'age', 'marital'])['education'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index(name='mode_education')

# Imputar la media para 'age'
for index, row in bank[bank['age'].isnull()].iterrows():
    mean_age = age_mean.loc[
        (age_mean['job'] == row['job']) &
        (age_mean['marital'] == row['marital']) &
        (age_mean['education'] == row['education']) , 'mean_age'
    ]

    if not mean_age.empty:
        bank.at[index, 'age'] = mean_age.values[0]

# Imputar la moda para 'education'
for index, row in bank[bank['education'].isnull()].iterrows():
    mode_education = education_mode.loc[
        (education_mode['job'] == row['job']) &
        (education_mode['age'] == row['age']) &
        (education_mode['marital'] == row['marital']), 'mode_education'
    ]

    if not mode_education.empty:
        bank.at[index, 'education'] = mode_education.values[0]

# Imputar la moda para 'marital'
for index, row in bank[bank['marital'].isnull()].iterrows():
    mode_marital = marital_mode.loc[
        (marital_mode['job'] == row['job']) &
        (marital_mode['age'] == row['age']) &
        (marital_mode['education'] == row['education']), 'mode_marital'
    ]

    if not mode_marital.empty:
        bank.at[index, 'marital'] = mode_marital.values[0]

# Verificar los valores nulos después de la imputación
print("Valores nulos en 'age' después de imputar:", bank['age'].isnull().sum())
print("Valores nulos en 'marital' después de imputar:", bank['marital'].isnull().sum())
print("Valores nulos en 'education' después de imputar:", bank['education'].isnull().sum())

# Contar el número de suscripciones y no suscripciones en la variable 'deposit'
num_depositos = bank[bank['deposit'] == 'yes'].shape[0]
num_no_depositos = bank[bank['deposit'] == 'no'].shape[0]

total = num_depositos + num_no_depositos
percent_depositos = (num_depositos / total) * 100
percent_no_depositos = (num_no_depositos / total) * 100
print(f"Porcentaje de depósitos: {percent_depositos:.2f}%")
print(f"Porcentaje de no depósitos: {percent_no_depositos:.2f}%")

# Gráficas de barras
plt.bar(["Depósitos (%d)" % num_depositos, "No Depósitos (%d)" % num_no_depositos],
        [num_depositos, num_no_depositos],
        color=["cyan", "red"],
        width=0.8)

plt.ylabel("Número de Personas")
plt.title("Distribución de Clientes Según la contratación de Depósitos")
plt.show()

"""Al realizar el borrado de outliers nuestras clases han quedado con mayor desbalanceo, por lo que procedo a realizar un oversampler"""

# Características (X) y la variable objetivo (y)
X = bank.drop(columns=['deposit'])
y = bank['deposit']

# Inicializar el oversampler
ros = RandomOverSampler(random_state=42)

# Aplicar oversampling
X_resampled, y_resampled = ros.fit_resample(X, y)

# Convertir de nuevo a un DataFrame
bank_resampled = pd.DataFrame(X_resampled, columns=X.columns)
bank_resampled['deposit'] = y_resampled

# Verificar el nuevo balance de clases
num_depositos_resampled = bank_resampled[bank_resampled['deposit'] == 'yes'].shape[0]
num_no_depositos_resampled = bank_resampled[bank_resampled['deposit'] == 'no'].shape[0]

print("Número de depósitos después de oversampling:", num_depositos_resampled)
print("Número de no depósitos después de oversampling:", num_no_depositos_resampled)

# Verificar los porcentajes
total_resampled = num_depositos_resampled + num_no_depositos_resampled
percent_depositos_resampled = (num_depositos_resampled / total_resampled) * 100
percent_no_depositos_resampled = (num_no_depositos_resampled / total_resampled) * 100

print(f"Porcentaje de depósitos después de oversampling: {percent_depositos_resampled:.2f}%")
print(f"Porcentaje de no depósitos después de oversampling: {percent_no_depositos_resampled:.2f}%")

"""#Preparacion de los datos

###Division del conjunto de datos

Nos aseguramos de mantener el balance de las clases utilizando el comando stratify
"""

# Definir X e Y
X = bank.drop('deposit', axis=1)  # Todas las columnas excepto 'deposit'
Y = bank['deposit']  # La variable objetivo 'deposit'

# Dividir los datos en conjuntos de entrenamiento y prueba (80% train, 20% test)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42,stratify=Y)

# Verificar el tamaño de cada conjunto
print(f"Tamaño del conjunto de entrenamiento (X): {X_train.shape}")
print(f"Tamaño del conjunto de prueba (X): {X_test.shape}")
print(f"Tamaño del conjunto de entrenamiento (Y): {Y_train.shape}")
print(f"Tamaño del conjunto de prueba (Y): {Y_test.shape}")

# Verificar la distribución de las clases
print("Distribución en el conjunto de entrenamiento:")
print(Y_train.value_counts(normalize=True))
print("Distribución en el conjunto de prueba:")
print(Y_test.value_counts(normalize=True))

"""###Codificación variables categoricas"""

#Columnas Binarias
binary_columns = [ 'default', 'housing', 'loan']

# Aplicar Label Encoding a las columnas binarias
label_encoder = LabelEncoder()


for column in binary_columns:
    X_train[column] = label_encoder.fit_transform(X_train[column])
    X_test[column] = label_encoder.transform(X_test[column])  # Aplicar la misma transformación al conjunto de prueba
    print(f"Valores únicos en '{column}' después de Label Encoding: {X_train[column].unique()}\n")

X_train.head()

encoder = OneHotEncoder(sparse_output=False)

# Define las columnas categóricas
categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']

one_hot_encoded_array_train = encoder.fit_transform(X_train[["poutcome", "marital", "education", "contact", "month", "job"]])
one_hot_encoded_array_test = encoder.transform(X_test[["poutcome", "marital", "education", "contact", "month", "job"]])

import joblib
import pickle
from sklearn.preprocessing import OneHotEncoder
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Guardar el encoder en un archivo .pkl
joblib.dump(encoder, '/content/drive/My Drive/encoder_bank.pkl')
print("Encoder guardado como encoder_bank.pkl")

# Convertir de nuevo a DataFrame
one_hot_encoded_df_train = pd.DataFrame(one_hot_encoded_array_train, columns=encoder.get_feature_names_out(["poutcome", "marital", "education", "contact", "month", "job"]))
one_hot_encoded_df_test = pd.DataFrame(one_hot_encoded_array_test, columns=encoder.get_feature_names_out(["poutcome", "marital", "education", "contact", "month", "job"]))

# Concatenar las columnas codificadas a los DataFrames originales
X_train = pd.concat([X_train.reset_index(drop=True), one_hot_encoded_df_train.reset_index(drop=True)], axis=1).drop(["poutcome", "marital", "education", "contact", "month", "job"], axis=1)
X_test = pd.concat([X_test.reset_index(drop=True), one_hot_encoded_df_test.reset_index(drop=True)], axis=1).drop(["poutcome", "marital", "education", "contact", "month","job"], axis=1)

X_train.head()

"""###Estandarizacion de datos"""

# Seleccionar las columnas numéricas
numerical_columns = ['age', 'balance', 'duration', 'pdays', 'campaign']

# Aplicar StandardScaler solo a esas columnas
scaler = StandardScaler()

# Ajustar el escalador solo en el conjunto de entrenamiento y transformar
X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])

# Aplicar la misma transformación al conjunto de prueba
X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])

X_train.head()

from IPython.display import display

# Imprimir las dimensiones de los conjuntos
print(f"Tamaño del conjunto de entrenamiento (X): {X_train.shape}")
print(f"Tamaño del conjunto de prueba (X): {X_test.shape}")
print(f"Tamaño del conjunto de entrenamiento (Y): {Y_train.shape}")
print(f"Tamaño del conjunto de prueba (Y): {Y_test.shape}")

print("\nConjunto de entrenamiento (X):")
display(X_train.head())

print("\nConjunto de entrenamiento (Y):")
display(Y_train.head())

print("\nConjunto de prueba (X):")
display(X_test.head())

print("\nConjunto de prueba (Y):")
display(Y_test.head())

"""# Modelo K-means
## Segmentación de clientes para personalización de campañas de marketing.

1. Buscamos el numero Optimo de Clusters
"""

# Definir el rango de valores de K
K_range = range(2, 11)  # Empezar desde 2 porque el coeficiente de silueta no se puede calcular para 1
inertia_values = []
silhouette_values = []

# Calcular la inercia y el coeficiente de silueta para cada valor de K
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(X_train)  # Asegúrate de que X_train esté definido

    inertia_values.append(kmeans.inertia_)

    # Calcular el coeficiente de silueta
    cluster_labels = kmeans.labels_
    silhouette_avg = silhouette_score(X_train, cluster_labels)
    silhouette_values.append(silhouette_avg)

# Graficar el método del codo
plt.figure(figsize=(12, 6))

# Gráfico de inercia
plt.subplot(1, 2, 1)
plt.plot(K_range, inertia_values, marker='o')
plt.title('Método del Codo para K-means')
plt.xlabel('Número de Clusters (K)')
plt.ylabel('Inercia')
plt.xticks(K_range)
plt.grid()

# Gráfico de silueta
plt.subplot(1, 2, 2)
plt.plot(K_range, silhouette_values, marker='o', color='orange')
plt.title('Coeficiente de Silueta para K-means')
plt.xlabel('Número de Clusters (K)')
plt.ylabel('Coeficiente de Silueta')
plt.xticks(K_range)
plt.grid()

plt.tight_layout()
plt.show()

"""Método del Codo: El "codo" es el punto donde la reducción en la inercia comienza a desacelerarse, es decir, donde ya no hay una disminución significativa. En este grafico parece que el codo se encuentra alrededor de
K = 3 o K =4.

Coeficiente de Silueta: Este gráfico muestra que los valores de silueta son más altos para  K = 2 y k= 3 y disminuyen a medida que aumenta K. Un valor más alto indica una mejor agrupación.

En resumen, basándome en los gráficos el valor optimo de K parece ser 3.
"""

# Definir el número de clusters optimo
k_optimo = 3

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Entrenar el modelo K-means en el conjunto de entrenamiento
kmeans = KMeans(n_clusters=k_optimo, random_state=0)
kmeans.fit(X_train_scaled)

# Predecir los clusters para el conjunto de entrenamiento y prueba
train_clusters = kmeans.predict(X_train_scaled)
#test_clusters = kmeans.predict(X_test)

# Agregar las etiquetas de clusters
X_train['Cluster'] = train_clusters
#X_test['Cluster'] = test_clusters

# Ver las primeras filas del conjunto de entrenamiento con la columna de cluster asignada
X_train.head()

X_train.columns

# Reducir la dimensionalidad a 2D para visualización
pca = PCA(n_components=2)
X_train_reduced = pca.fit_transform(X_train_scaled)

# Crear un DataFrame para facilitar la visualización
df_reduced = pd.DataFrame(data=X_train_reduced, columns=['Componente 1', 'Componente 2'])
df_reduced['Cluster'] = X_train['Cluster']

# Configurar el estilo del gráfico
sns.set(style="whitegrid")

# Crear el scatter plot
plt.figure(figsize=(12, 8))
scatter = sns.scatterplot(data=df_reduced, x='Componente 1', y='Componente 2',
                          hue='Cluster', palette='viridis', s=100, alpha=0.7, edgecolor='w')

# Ajustar límites de los ejes
plt.xlim(df_reduced['Componente 1'].min() - 1, df_reduced['Componente 1'].max() + 1)
plt.ylim(df_reduced['Componente 2'].min() - 1, df_reduced['Componente 2'].max() + 1)

# Añadir título y etiquetas
plt.title('Visualización de Clusters usando K-means', fontsize=20)
plt.xlabel('Componente 1', fontsize=16)
plt.ylabel('Componente 2', fontsize=16)

# Mostrar leyenda
plt.legend(title='Clusters', fontsize=12)
plt.grid()
plt.show()

"""##Interpretacion de los resultados:

La segmentación de clientes mediante machine learning permite a la empresa obtener insights valiosos que se alinean con sus objetivos de negocio.

Al analizar los tres clusters bien definidos, observamos grupos distintos de clientes que revelan diferencias significativas en comportamiento y características. Cada cluster puede representar un patrón de gasto diferente: por ejemplo, un grupo puede incluir clientes de alto gasto, mientras que otro puede abarcar a clientes más conservadores. Esta segmentación ayuda a identificar nichos de mercado y oportunidades para lanzar nuevos productos adaptados a cada grupo, **lo que responde al objetivo de mejorar la personalización de ofertas y campañas de marketing**.

La existencia de clusters bien definidos indica que la segmentación ha sido exitosa, permitiendo a la empresa implementar estrategias personalizadas. Esto está directamente relacionado con el objetivo de **optimizar la asignación de recursos y maximizar el retorno de la inversión a través de campañas dirigidas y ajustadas a las necesidades de cada segmento**.

Con esta información, la empresa puede adaptar sus productos y servicios para maximizar su atractivo. Esto facilita la personalización de campañas, creando mensajes y ofertas que resuenen con cada grupo, lo que mejora la tasa de respuesta y efectividad de las campañas. Al comprender mejor las necesidades de cada cluster, la empresa puede innovar en su oferta y desarrollar productos que satisfagan específicamente a cada segmento, incrementando así su cuota de mercado, lo que se alinea con el objetivo de **aumentar la retención de clientes**.

Finalmente, la capacidad de ajustar estrategias y productos según la evolución del comportamiento de los clientes asegurará que la empresa se mantenga relevante y competitiva en el mercado.

#Deployment del Modelo
"""

import pickle

# Guardar el modelo y el escalador usando pickle
model_path = '/content/drive/My Drive/kmeans_model_bank.pkl'  # Ajusta la ruta según sea necesario
scaler_path = '/content/drive/My Drive/scaler_bank.pkl'  # Ajusta la ruta según sea necesario

with open(model_path, 'wb') as model_file:
    pickle.dump(kmeans, model_file)

with open(scaler_path, 'wb') as scaler_file:
    pickle.dump(scaler, scaler_file)

pip install streamlit

# Rutas de los modelos y el escalador
model_path = '/content/drive/My Drive/kmeans_model_bank.pkl'  # Ruta del modelo
scaler_path = '/content/drive/My Drive/scaler_bank.pkl'        # Ruta del escalador

# Cargar el modelo y el escalador
try:
    model = joblib.load(model_path)
    scaler = joblib.load(scaler_path)
except FileNotFoundError as e:
    st.error(f"Error: {e}")
except Exception as e:
    st.error(f"Error al cargar los modelos: {e}")

# Interfaz de usuario
st.title("Predicción de Clientes del Banco")

# Entradas del usuario
job = st.selectbox('Job', ['admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'student', 'technician', 'unemployed', 'unknown'])
marital = st.selectbox('Marital Status', ['divorced', 'married', 'single', 'unknown'])
education = st.selectbox('Education', ['primary', 'secondary', 'tertiary', 'unknown'])
default = st.selectbox('Credit Default?', ['yes', 'no'])
housing = st.selectbox('Housing Loan?', ['yes', 'no'])
loan = st.selectbox('Personal Loan?', ['yes', 'no'])
contact = st.selectbox('Contact Communication Type', ['cellular', 'telephone'])
poutcome = st.selectbox('Previous Outcome', ['failure', 'nonexistent', 'success'])
age = st.number_input('Age', min_value=18, max_value=100, step=1)
balance = st.number_input('Balance', min_value=0, step=100)
duration = st.number_input('Duration', min_value=0, step=10)
pdays = st.number_input('Pdays', min_value=-1, step=1)
campaign = st.number_input('Campaign', min_value=1, step=1)

# Preparar los datos para la predicción
# Crear DataFrame con las columnas categóricas y numéricas
input_data = pd.DataFrame({
    'job': [job], 'marital': [marital], 'education': [education],
    'default': [default], 'housing': [housing], 'loan': [loan],
    'contact': [contact], 'poutcome': [poutcome],
    'age': [age], 'balance': [balance], 'duration': [duration],
    'pdays': [pdays], 'campaign': [campaign]
})

# Separar columnas categóricas y numéricas
categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']
numerical_columns = ['age', 'balance', 'duration', 'pdays', 'campaign']

# Aplicar One-Hot Encoding a las columnas categóricas
try:
    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
    encoded_categorical_data = encoder.fit_transform(input_data[categorical_columns])
    encoded_categorical_df = pd.DataFrame(encoded_categorical_data, columns=encoder.get_feature_names_out(categorical_columns))

    # Combinar con las columnas numéricas
    input_data_final = pd.concat([encoded_categorical_df, input_data[numerical_columns]], axis=1)

    # Escalar las columnas numéricas
    input_data_final[numerical_columns] = scaler.transform(input_data_final[numerical_columns])
except Exception as e:
    st.error(f"Error al procesar los datos: {e}")

# Realizar predicción
if st.button('Realizar Predicción'):
    try:
        prediction = model.predict(input_data_final)
        if prediction[0] == 1:
            st.success('El cliente probablemente suscriba el préstamo.')
        else:
            st.warning('El cliente probablemente no suscriba el préstamo.')
    except Exception as e:
        st.error(f"Error en la predicción: {e}")

# Crear y guardar el archivo app.py
with open('/content/drive/My Drive/app.py', 'w') as f:
    f.write("""

import streamlit as st
import joblib
import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# Rutas de los modelos y el escalador
model_path = '/content/drive/My Drive/kmeans_model_bank.pkl'  # Ruta del modelo
scaler_path = '/content/drive/My Drive/scaler_bank.pkl'        # Ruta del escalador

# Cargar el modelo y el escalador
try:
    model = joblib.load(model_path)
    scaler = joblib.load(scaler_path)
except FileNotFoundError as e:
    st.error(f"Error: {e}")
except Exception as e:
    st.error(f"Error al cargar los modelos: {e}")

# Interfaz de usuario
st.title("Predicción de Clientes del Banco")

# Entradas del usuario
job = st.selectbox('Job', ['admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'student', 'technician', 'unemployed', 'unknown'])
marital = st.selectbox('Marital Status', ['divorced', 'married', 'single', 'unknown'])
education = st.selectbox('Education', ['primary', 'secondary', 'tertiary', 'unknown'])
default = st.selectbox('Credit Default?', ['yes', 'no'])
housing = st.selectbox('Housing Loan?', ['yes', 'no'])
loan = st.selectbox('Personal Loan?', ['yes', 'no'])
contact = st.selectbox('Contact Communication Type', ['cellular', 'telephone'])
poutcome = st.selectbox('Previous Outcome', ['failure', 'nonexistent', 'success'])
age = st.number_input('Age', min_value=18, max_value=100, step=1)
balance = st.number_input('Balance', min_value=0, step=100)
duration = st.number_input('Duration', min_value=0, step=10)
pdays = st.number_input('Pdays', min_value=-1, step=1)
campaign = st.number_input('Campaign', min_value=1, step=1)

# Preparar los datos para la predicción
# Crear DataFrame con las columnas categóricas y numéricas
input_data = pd.DataFrame({
    'job': [job], 'marital': [marital], 'education': [education],
    'default': [default], 'housing': [housing], 'loan': [loan],
    'contact': [contact], 'poutcome': [poutcome],
    'age': [age], 'balance': [balance], 'duration': [duration],
    'pdays': [pdays], 'campaign': [campaign]
})

# Separar columnas categóricas y numéricas
categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']
numerical_columns = ['age', 'balance', 'duration', 'pdays', 'campaign']

# Aplicar One-Hot Encoding a las columnas categóricas
try:
    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
    encoded_categorical_data = encoder.fit_transform(input_data[categorical_columns])
    encoded_categorical_df = pd.DataFrame(encoded_categorical_data, columns=encoder.get_feature_names_out(categorical_columns))

    # Combinar con las columnas numéricas
    input_data_final = pd.concat([encoded_categorical_df, input_data[numerical_columns]], axis=1)

    # Escalar las columnas numéricas
    input_data_final[numerical_columns] = scaler.transform(input_data_final[numerical_columns])
except Exception as e:
    st.error(f"Error al procesar los datos: {e}")

# Realizar predicción
if st.button('Realizar Predicción'):
    try:
        prediction = model.predict(input_data_final)
        if prediction[0] == 1:
            st.success('El cliente probablemente suscriba el préstamo.')
        else:
            st.warning('El cliente probablemente no suscriba el préstamo.')
    except Exception as e:
        st.error(f"Error en la predicción: {e}")
        """)

streamlit run app.py